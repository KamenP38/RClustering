library(rpart.plot)
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rpart")
library(rpart)
library(rpart.plot)
rm(list=ls())
setwd("/home/vkg/IIT/CS422/lectures/lecture-4")
df <- read.csv("loan.csv", header=T, sep=",")
model <- rpart(Defaulted ~ ., data = df,
control=rpart.control(minsplit=2, minbucket=1))
rpart.plot(model, extra=103, fallen.leaves = T, type=2,
main="RPART Decision Tree")
# Let's see how it predicts.
obs <- data.frame(Homeowner="Yes", Marital.Status="Single",
Annual.Income=200)
test.df <- obs
obs <- data.frame(Homeowner="No", Marital.Status="Divorced",
Annual.Income=160)
test.df <- rbind(test.df, obs)
obs <- data.frame(Homeowner="No", Marital.Status="Married",
Annual.Income=73)
test.df <- rbind(test.df, obs)
# Predict the class and the probability associated with each prediction
pred.class <- predict(model, newdata=test.df, type = "class")
pred.prob <- predict(model, newdata=test.df, type = "prob")
---
title: "CS422: Decision Trees using rpart"
output: html_notebook
author: "Vijay K. Gurbani, Ph.D., Illinois Institute of Technology"
---
##### Make sure you install the following packages:
##### > install.packages(c("rpart", "rpart.plot", "caret", "e1071", "knitr", "ROCR"))
# Decision Tree construction and introspection
```{r}
library(rpart)
library(caret)
library(rpart.plot)
library(ROCR)
rm(list=ls())
setwd("/home/vkg/IIT/CS422/lectures/lecture-5/")
options("digits"=3)
# Load the Diabetes dataset
diabetes <- read.csv("diabetes-mod.csv", header=T, sep=",", comment.char = '#')
head(diabetes)
```
#### Split the data in a 80-20 (train-test) ratio.
```{r}
set.seed(100)
index <- sample(1:nrow(diabetes), size=0.2*nrow(diabetes))
test <- diabetes[index, ]
train <- diabetes[-index, ]
```
#### Build the decision tree model using all predictor variables.  If you only wanted to use, say, preg and plas predictor variables, you would instead specify the formula as **label ~ preg+plas**.
#### Note that we build the model on the training data *(data = train)*
```{r}
model <- rpart(label ~ ., method="class", data=train)
```
#### Visualize the decision tree
<a id="plot_initial_tree"></a>
```{r plot_initial_tree}
rpart.plot(model, extra=104, fallen.leaves = T, type=4, main="Rpart on Diabetes data (Full Tree)")
```
#### Inspect the model you created for more information.
```{r}
summary(model)
```
# Decision tree prediction
#### Now, let's run predictions using the fitted model.  Recall that we have a test dataset that contains 20% of our data.  We use the fitted model on this test dataset to see how well the model works.
```{r}
pred <- predict(model, test, type="class")
```
#### pred now contains the predictions; if we take a look at a few of them, this is what we will see:
```{r}
head(pred)
```
#### Above, the first line is the index of the observation relative to the diabetes dataset (i.e., the first observation in the test dataset corresponds to the 166th observation in the diabetes dataset).  Our model is predicting that the first observation of the test dataset will be predicted as 0, the second observation as 1, and so on.
#### Let's see the true labels in the test dataset:
```{r}
head(test$label)
```
#### Hmmm ... looks like the model matched 3 predictions out of the 6 shown.
#### Let's create a confusion matrix.  We will do so in two ways.  First, we will create the confusion matrix manually, and then we will use the package caret's confusionMatrix() method.
```{r}
# How did we do?  Let's create a confusion matrix manually first.  Let's figure
# out how many observations in our test set were positive and negative.  These
# are the true labels.
total_pos <- sum(test[,9] == 1)
total_neg <- sum(test[,9] == 0)
# Now, let's see how many of the predicted observations that were positive
# actually matched the true labels.
tp <- sum(test[,9] == 1 & pred == 1)
tn <- sum(test[,9] == 0 & pred == 0)
table(pred, test[,9]) # Manual confusion matrix
```
#### And here is the confusion matrix from caret package.  Note that this confusion matrix gives us a lot of other information (sensitivity, accuracy, specificity, ...)
<a id="confusion_matrix_full_tree"></a>
```{r}
confusionMatrix(pred, as.factor(test[, 9]))
```
#### Here is the ROC curve for the test data.  We will also print the AUC at the end.
<a id="pred_chunk"></a>
```{r pred_chunk}
# ROC curve
pred.rocr <- predict(model, newdata=test, type="prob")[,2]
f.pred <- prediction(pred.rocr, test$label)
f.perf <- performance(f.pred, "tpr", "fpr")
plot(f.perf, colorize=T, lwd=3)
abline(0,1)
auc <- performance(f.pred, measure = "auc")
cat(paste("The area under curve (AUC) for this model is ", round(auc@y.values[[1]], 3)))
```
#### In the [code chunk](#pred_chunk) above, note that when we call predict(), we specify the parameter *type="prob"*.  This, then, returns us the probability of how confident the model is of its prediction.  Let's print the first few probabilities.  To make the output attractive, I have created a data frame that has 3 columns: True label, Predicted label, and probability.
```{r}
tmp.df <- data.frame(True_Label=head(test$label),
Predicted_Label=head(pred),
Probability=head(pred.rocr))
tmp.df
```
```{r echo=FALSE}
rm(tmp.df)
```
# Decision Tree pruning
#### To prune the tree, first display the complexity table.
```{r}
options("digits"=5)
printcp(model)
plotcp(model)
```
```{r echo=FALSE}
options("digits"=3)
```
#### In the output above, note the *xerror* (cross-validation) column. Note how it decreases until the 5th row, and then increases.  We want to get the complexity value (*CP* column) associated with the smallest *xerror*.  The following code does this:
```{r}
cpx=model$cptable[which.min(model$cptable[,"xerror"]), "CP"]
cpx
```
#### That is the value of complexity we require when we prune the tree.  So, let's prune the tree and plot it to see if it is less complex ...
```{r}
ptree <- prune(model, cp=cpx)
rpart.plot(ptree, extra=104, fallen.leaves = T, type=4, main="Pruned Diabetes Decision Tree")
```
#### Hmmm, the tree is a bit less complex than the [initial tree](#plot_initial_tree) we had.  So far, so good.
#### How does the pruned tree perform on predictions?  Let's see...
```{r}
ptree.pred <- predict(ptree, test, type="class")
confusionMatrix(ptree.pred, as.factor(test[, 9]))
```
#### Looks like we have done better than before.  In our [earlier confusion matrix](#confusion_matrix_full_tree), we had an accuracy of 0.748, sensitivity of 0.709 and specificity of 0.788.  In our pruned tree, we have an increased accuracy (0.776), increased sensitivity (0.782), though a decreased specificity (0.769), i.e., we have increased our TPs at the expense of our TN.
#### And finally, let's see if we improved our AUC with the pruned tree:
```{r}
pruned.pred.rocr <- predict(ptree, newdata=test, type="prob")[,2]
f.pruned.pred <- prediction(pruned.pred.rocr, test$label)
auc.pruned <- performance(f.pruned.pred, measure = "auc")
cat(paste("The area under curve (AUC) for the prunded tree is ", round(auc.pruned@y.values[[1]], 3)))
```
#### We see a slight improvement for AUC in the prunded tree.  Before, the AUC was 0.835, it is now 0.837.
options(digits = 3)
x <- c(1000, 980, 1201, 600, 5000)
y <- c(0, -1, 2, 0.50, 10)
data <- data.frame(x,y)
data
plot(data, main="Raw data")
text(data, pos=2, labels=c("P1", "P2", "P3", "P4", "P5"))
data.centered <- scale(data, scale=F)
?scale()
plot(data.centered, main="Centered data")
text(data.centered, pos=2, labels=c("P1", "P2", "P3", "P4", "P5"))
data.scaled <- scale(data, center=F)
plot(data.scaled, main="Scaled data")
text(data.scaled, pos=2, labels=c("P1", "P2", "P3", "P4", "P5"))
data.standardized <- scale(data) # (x - mean(x))/sd(x)
plot(data.standardized, main="Standardized (mean = 0, sd = 1)")
text(data.standardized, pos=2, labels=c("P1", "P2", "P3", "P4", "P5"))
# Note that scaling, transforming, etc. does not change the density curve
plot(density(data$x))
plot(density(data.scaled[,1]))
# Note that scaling, transforming, etc. does not change the density curve
plot(density(data$x))
plot(density(data.scaled[,1]))
plot(density(data.centered[,1]))
plot(density(data.standardized[,1]))
dist(data, method="euclidean", diag = T, upper=T)
dist(data.centered, method="euclidean", diag = T, upper=T)
dist(data.scaled, method="euclidean", diag = T, upper=T)
dist(data.standardized, method="euclidean", diag = T, upper=T)
options(digits=3)
x <- c(0,2,3,5)
y <- c(2,0,1,1)
df <- data.frame(x,y)
# Plot it.
plot(df, bg="lightgreen", pch=21, xlim=c(0,6), ylim=c(0,3))
# See http://www.endmemo.com/program/R/pchsymbols.php for PCH symbols.
text(x,y, pos=4, labels=c("P1","P2","P3","P4"))
d <- dist(df, method="euclidean", upper=T, diag=T) # L2
d
d <- dist(df, method="manhattan", upper=T, diag=T) # L1
d
d <- dist(df, method="maximum", upper=T, diag=T) # L_max
d
library(dplyr)
library(psych)
library(factoextra)
library(dbscan)
library(fpc)
setwd("C:/Users/ACER/Desktop/Desktop/IIT_Stuff/CS 422/myHws/HW8")
# Data that we're going to use
df <- read.table("file19.txt", skip = 20, header = TRUE)
# Show part of data
head(df)
# Find correlation between the attribute
cor(df[,-1])
# Drop c,p,m since they are strongly correlated with C,P,M - we don't need both
df$c = NULL
df$p = NULL
df$m = NULL
summary(df)
#Standardize the data
df.scale <- scale(df[,-1])
summary(df.scale)
# In this case standardization might not be necessary since the difference between min and max is not too big.
# However, for the sake of making the work easier for the computer (algorithm is easier to use), I will scale.
write.csv(df,"file19.csv")
# B) Clustering
# i) Determine how many clusters are needed
?fviz_nbclust()
fviz_nbclust(df.scale[, -1], kmeans, method = "wss")
fviz_nbclust(df.scale[, -1], kmeans, method = "silhouette")
? fviz_cluster()
clst <- kmeans(df.scale[, -1], centers = 8, nstart = 25)
fviz_cluster(clst, df.scale[, -1], main = "Cluster")
clst$size
# (iv) What is the total SSE of the clusters?
clst$totss
clst$tot.withinss
# (v) What is the SSE of each cluster?
clst$withinss
df[which(clst$cluster == 1),]
df[which(clst$cluster == 2),]
df[which(clst$cluster == 3),]
df[which(clst$cluster == 4),]
df[which(clst$cluster == 5),]
df[which(clst$cluster == 6),]
df[which(clst$cluster == 7),]
df[which(clst$cluster == 8),]
library(cluster)
library(factoextra)
rm(list=ls())
#       1     2     3     4     5     6     7    8
x <- c(210,  195,  180,  200,  235,  110,   90, 126)  # Weight in lbs
y <- c(5.08, 4.03, 6.00, 5.02, 5.07, 5.03, 4.0, 5.01) # Height in feets
# and inches
# Remember from previous lecture: location of points in a 2-d space
# does not change even after scaling.  In a sense, scaling "adjusts"
# the coordinates.  So, with that ...
plot(x, y, main="Raw Data")
text(x,y, pos=2, labels=c("1", "2", "3", "4", "5", "6", "7", "8"))
df <- data.frame(weight=x, height=y)
df
df.scaled <- scale(df)
k <- kmeans(df, centers=2)
k.scaled <- kmeans(df.scaled, centers=2)
# Now let's see what the clusters looks like, scaled and unscaled.
fviz_cluster(k, data=df, main="Unscaled clusters")
fviz_cluster(k.scaled, data=df.scaled, main="Scaled clusters")
# Let's look at their distances, for scaled and unscaled versions.
# Notice the distance from point 5 to points 6, 7, 8.  5's 'x' attribute
# tends to dominate the 'x' attributes of 6,7,8.
dist(data.frame(x=x, y=y), method="euclidean", upper=T)
dist(scale(data.frame(x=x, y=y)), method="euclidean", upper=T)
# And finally, let's see what's contained in the results returned
# by kmeans()
print(k.scaled)
# Examine k.scaled${cluster, size, centers, totss, withinss}
totss = tot.withinss + betweenss
# Examine k.scaled${cluster, size, centers, totss, withinss}
totss <- tot.withinss + betweenss
?scale()
library(factoextra)
rm(list=ls())
#       1     2     3     4     5     6     7    8
x <- c(210,  195,  180,  200,  235,  110,   90, 126)  # Weight in lbs
y <- c(5.08, 4.03, 6.00, 5.02, 5.07, 5.03, 4.0, 5.01) # Height in feets
# and inches
# Remember from previous lecture: location of points in a 2-d space
# does not change even after scaling.  In a sense, scaling "adjusts"
# the coordinates.  So, with that ...
plot(x, y, main="Raw Data")
text(x,y, pos=2, labels=c("1", "2", "3", "4", "5", "6", "7", "8"))
df <- data.frame(weight=x, height=y)
df
df.scaled <- scale(df)
k <- kmeans(df, centers=2)
k.scaled <- kmeans(df.scaled, centers=2)
# Now let's see what the clusters looks like, scaled and unscaled.
fviz_cluster(k, data=df, main="Unscaled clusters")
fviz_cluster(k.scaled, data=df.scaled, main="Scaled clusters")
# Let's look at their distances, for scaled and unscaled versions.
# Notice the distance from point 5 to points 6, 7, 8.  5's 'x' attribute
# tends to dominate the 'x' attributes of 6,7,8.
dist(data.frame(x=x, y=y), method="euclidean", upper=T)
dist(scale(data.frame(x=x, y=y)), method="euclidean", upper=T)
# Let's look at their distances, for scaled and unscaled versions.
# Notice the distance from point 5 to points 6, 7, 8.  5's 'x' attribute
# tends to dominate the 'x' attributes of 6,7,8.
dist(data.frame(x=x, y=y), method="euclidean", upper=T)
dist(scale(data.frame(x=x, y=y)), method="euclidean", upper=T)
rm(list=ls())
options(digits=0) # Round up the numbers
setwd("/home/vkg/IIT/CS422/lectures/clustering")
data <- read.csv("distances.csv", sep=",", header=T, comment.char = "#")
dist(data[,2:8], diag = T, upper = T)
c.single <- hclust(dist(data[,2:8]), method="single")
plot(c.single, labels=c("ORD", "LAS", "DFW", "JFK", "ATL", "MSP", "PHL"))
c.complete <- hclust(dist(data[,2:8]), method="complete")
plot(c.complete, labels=c("ORD", "LAS", "DFW", "JFK", "ATL", "MSP", "PHL"))
library(cluster)
library(factoextra)
rm(list=ls())
options(digits=0) # Round up the numbers
setwd("/home/vkg/IIT/CS422/lectures/clustering")
data <- read.csv("distances.csv", sep=",", header=T, comment.char = "#")
dist(data[,2:8], diag = T, upper = T)
c.single <- hclust(dist(data[,2:8]), method="single")
plot(c.single, labels=c("ORD", "LAS", "DFW", "JFK", "ATL", "MSP", "PHL"))
c.complete <- hclust(dist(data[,2:8]), method="complete")
plot(c.complete, labels=c("ORD", "LAS", "DFW", "JFK", "ATL", "MSP", "PHL"))
install.packages('stats')
library('stats')
library('stats')
rm(list=ls())
options(digits=0) # Round up the numbers
setwd("/home/vkg/IIT/CS422/lectures/clustering")
data <- read.csv("distances.csv", sep=",", header=T, comment.char = "#")
setwd("C:/Users/ACER/Desktop/Desktop/IIT_Stuff/CS 422")
data <- read.csv("distances.csv", sep=",", header=T, comment.char = "#")
setwd("C:/Users/ACER/Desktop/Desktop/IIT_Stuff/CS 422/myHws")
data <- read.csv("distances.csv", sep=",", header=T, comment.char = "#")
dist(data[,2:8], diag = T, upper = T)
c.single <- hclust(dist(data[,2:8]), method="single")
plot(c.single, labels=c("ORD", "LAS", "DFW", "JFK", "ATL", "MSP", "PHL"))
c.complete <- hclust(dist(data[,2:8]), method="complete")
plot(c.complete, labels=c("ORD", "LAS", "DFW", "JFK", "ATL", "MSP", "PHL"))
c.average <- hclust(dist(data[,2:8]), method="average")
plot(c.average, labels=c("ORD", "LAS", "DFW", "JFK", "ATL", "MSP", "PHL"))
library(dplyr)
library(psych)
library(cluster)
library(factoextra)
library(dbscan)
library(fpc)
setwd("C:/Users/ACER/Desktop/Desktop/IIT_Stuff/CS 422/myHws/HW8")
# Data that we're going to use
df <- read.table("file19.txt", skip = 20, header = TRUE)
# Show part of data
head(df)
# Find correlation between the attribute
cor(df[,-1])
# Drop c,p,m since they are strongly correlated with C,P,M - we don't need both
df$c = NULL
df$p = NULL
df$m = NULL
summary(df)
#Standardize the data
df.scale <- scale(df[,-1])
summary(df.scale)
?scale()
# In this case standardization might not be necessary since the difference between min and max is not too big.
# However, for the sake of making the work easier for the computer (algorithm is easier to use), I will scale.
write.csv(df,"file19.csv")
# B) Clustering
# i) Determine how many clusters are needed
?fviz_nbclust()
fviz_nbclust(df.scale[, -1], kmeans, method = "wss")
fviz_nbclust(df.scale[, -1], kmeans, method = "silhouette")
library(factoextra)
rm(list=ls())
#       1     2     3     4     5     6     7    8
x <- c(210,  195,  180,  200,  235,  110,   90, 126)  # Weight in lbs
y <- c(5.08, 4.03, 6.00, 5.02, 5.07, 5.03, 4.0, 5.01) # Height in feets
# and inches
# Remember from previous lecture: location of points in a 2-d space
# does not change even after scaling.  In a sense, scaling "adjusts"
# the coordinates.  So, with that ...
plot(x, y, main="Raw Data")
text(x,y, pos=2, labels=c("1", "2", "3", "4", "5", "6", "7", "8"))
df <- data.frame(weight=x, height=y)
df.scaled <- scale(df)
k <- kmeans(df, centers=2)
k.scaled <- kmeans(df.scaled, centers=2)
# Now let's see what the clusters looks like, scaled and unscaled.
fviz_cluster(k, data=df, main="Unscaled clusters")
fviz_cluster(k.scaled, data=df.scaled, main="Scaled clusters")
k.scaled <- kmeans(df.scaled, centers=3)
# Now let's see what the clusters looks like, scaled and unscaled.
fviz_cluster(k, data=df, main="Unscaled clusters")
fviz_cluster(k.scaled, data=df.scaled, main="Scaled clusters")
k.scaled <- kmeans(df.scaled, centers=4)
# Now let's see what the clusters looks like, scaled and unscaled.
fviz_cluster(k, data=df, main="Unscaled clusters")
fviz_cluster(k.scaled, data=df.scaled, main="Scaled clusters")
k.scaled <- kmeans(df.scaled, centers=7)
# Now let's see what the clusters looks like, scaled and unscaled.
fviz_cluster(k, data=df, main="Unscaled clusters")
fviz_cluster(k.scaled, data=df.scaled, main="Scaled clusters")
library(dplyr)
library(psych)
library(cluster)
library(factoextra)
library(dbscan)
library(fpc)
setwd("C:/Users/ACER/Desktop/Desktop/IIT_Stuff/CS 422/myHws/HW8")
# Data that we're going to use
df <- read.table("file19.txt", skip = 20, header = TRUE)
# Show part of data
head(df)
# Find correlation between the attribute
cor(df[,-1])
# Drop c,p,m since they are strongly correlated with C,P,M - we don't need both
df$c = NULL
df$p = NULL
df$m = NULL
summary(df)
#Standardize the data
df.scale <- scale(df[,-1])
summary(df.scale)
?scale()
# In this case standardization might not be necessary since the difference between min and max is not too big.
# However, for the sake of making the work easier for the computer (algorithm is easier to use), I will scale.
write.csv(df,"file19.csv")
# B) Clustering
# i) Determine how many clusters are needed
?fviz_nbclust()
fviz_nbclust(df.scale[, -1], kmeans, method = "wss")
fviz_nbclust(df.scale[, -1], kmeans, method = "silhouette")
? fviz_cluster()
clst <- kmeans(df.scale[, -1], centers = 8, nstart = 25)
fviz_cluster(clst, df.scale[, -1], main = "Cluster")
# I chose to have 8 clusters because from the wss graph, we can notice that after 8, there isn't a big change
# in the numbers. Perhaps, ideally we would use 10.
# (iii) How many observations are in each cluster?
clst$size
# Number of observations in each cluster: 1, 8, 7, 11, 6, 8, 13, 12
# (iv) What is the total SSE of the clusters?
# totss = tot.withinss + betweenss
clst$totss
clst$tot.withinss
# Total SSE of the clusters = 260
# Total within-clusters = 17.57414
# (v) What is the SSE of each cluster?
clst$withinss
#  0.000000e+00 5.908626e-01 3.882675e-31 1.841650e+00 5.083201e+00 1.471193e+00 1.869983e+00 6.717249e+00
df[which(clst$cluster == 1),]
df[which(clst$cluster == 2),]
df[which(clst$cluster == 3),]
df[which(clst$cluster == 4),]
df[which(clst$cluster == 5),]
df[which(clst$cluster == 6),]
df[which(clst$cluster == 7),]
df[which(clst$cluster == 8),]
# (iii) How many observations are in each cluster?
clst$size
source("C:/Users/ACER/Desktop/Desktop/IIT_Stuff/CS 422/myHws/HW8/kamen-petkov-hw8.rmd")
library(dplyr)
library(psych)
library(cluster)
library(factoextra)
library(dbscan)
library(fpc)
setwd("C:/Users/ACER/Desktop/Desktop/IIT_Stuff/CS 422/myHws/HW8")
# Data that we're going to use
df <- read.table("file19.txt", skip = 20, header = TRUE)
# Show part of data
head(df)
# Find correlation between the attribute
cor(df[,-1])
# (i)
df$c = NULL
df$p = NULL
df$m = NULL
summary(df)
# (ii) Standardize the data
df.scale <- scale(df[,-1])
summary(df.scale)
# (iii)
write.csv(df,"file19.csv")
fviz_nbclust(df.scale[, -1], kmeans, method = "wss")
fviz_nbclust(df.scale[, -1], kmeans, method = "silhouette")
# (ii) Once you have determined the number of clusters, run k-means clustering on the dataset to create that many
# clusters. Plot the clusters using fviz_cluster().
clst <- kmeans(df.scale[, -1], centers = 8, nstart = 25)
fviz_cluster(clst, df.scale[, -1], main = "Cluster")
# (iii) How many observations are in each cluster?
clst$size
# (iv) What is the total SSE of the clusters?
clst$totss
clst$tot.withinss
# Total SSE of the clusters = 260
# Total within-clusters = 17.57414
# (v) What is the SSE of each cluster?
clst$withinss
# (vi) Perform an analysis of each cluster to determine how the mammals are grouped in each cluster, and whether
# that makes sense?
df[which(clst$cluster == 1),]
df[which(clst$cluster == 2),]
df[which(clst$cluster == 3),]
df[which(clst$cluster == 4),]
df[which(clst$cluster == 5),]
df[which(clst$cluster == 6),]
df[which(clst$cluster == 7),]
df[which(clst$cluster == 8),]
source("C:/Users/ACER/Desktop/Desktop/IIT_Stuff/CS 422/myHws/HW8/kamen-petkov-hw8.rmd")
